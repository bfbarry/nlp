{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(s):\n",
    "    return s.lower().split()\n",
    "\n",
    "documents = [ # a.k.a text corpus\n",
    "    \"Human machine interface for lab abc computer applications\",\n",
    "    \"A survey of user opinion of computer system response time\",\n",
    "    \"The EPS user interface management system\",\n",
    "    \"System and human system engineering testing of EPS\",\n",
    "    \"Relation of user perceived response time to error measurement\",\n",
    "    \"The generation of random binary unordered trees\",\n",
    "    \"The intersection graph of paths in trees\",\n",
    "    \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "    \"Graph minors A survey\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "[amount of training data](https://stackoverflow.com/questions/48059145/how-much-data-is-actually-required-to-train-a-doc2vec-model)\n",
    "\n",
    "# [Core Concepts](https://radimrehurek.com/gensim/auto_examples/core/run_core_concepts.html)\n",
    "### preprocessed corpus: *filtering by stopwords and token frequencies*\n",
    "- this is a bit simplistic\n",
    "\n",
    "also see [`simple_preprocess()`](https://radimrehurek.com/gensim/utils.html#gensim.utils.simple_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['human', 'interface', 'computer'],\n",
       " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
       " ['eps', 'user', 'interface', 'system'],\n",
       " ['system', 'human', 'system', 'eps'],\n",
       " ['user', 'response', 'time'],\n",
       " ['trees'],\n",
       " ['graph', 'trees'],\n",
       " ['graph', 'minors', 'trees'],\n",
       " ['graph', 'minors', 'survey']]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoplist = set('for a of the and to in'.split(' '))\n",
    "# remove stopwords\n",
    "texts = [[word for word in tokenize(document) if word not in stoplist] \n",
    "             for document in documents]\n",
    "\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "        \n",
    "clean_corpus = [[token for token in text if frequency[token] > 1] for text in texts]\n",
    "clean_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gensim dictionary\n",
    "[Construct word<->id mappings](https://radimrehurek.com/gensim/corpora/dictionary.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'computer': 0, 'human': 1, 'interface': 2, 'response': 3, 'survey': 4, 'system': 5, 'time': 6, 'user': 7, 'eps': 8, 'trees': 9, 'graph': 10, 'minors': 11}\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(clean_corpus)\n",
    "# dictionary.save('/tmp/deerwester.dict')  # store the dictionary, for future reference\n",
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transforming new docs\n",
    "Has to be tokenized first!\n",
    "\n",
    "`(token_id, token_count)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1)]\n"
     ]
    }
   ],
   "source": [
    "new_doc = \"Human computer interaction\"\n",
    "new_vec = dictionary.doc2bow(tokenize(new_doc))  \n",
    "print(new_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  vectorize Corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1)],\n",
       " [(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)],\n",
       " [(2, 1), (5, 1), (7, 1), (8, 1)],\n",
       " [(1, 1), (5, 2), (8, 1)],\n",
       " [(3, 1), (6, 1), (7, 1)],\n",
       " [(9, 1)],\n",
       " [(9, 1), (10, 1)],\n",
       " [(9, 1), (10, 1), (11, 1)],\n",
       " [(4, 1), (10, 1), (11, 1)]]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(toks) for toks in clean_corpus]\n",
    "bow_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "\"The tf-idf model transforms vectors from the bag-of-words representation to a vector space where the frequency counts are weighted according to the relative rarity of each word in the corpus.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5, 0.5898341626740045), (11, 0.8075244024440723)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import models\n",
    "\n",
    "tfidf = models.TfidfModel(bow_corpus) # this fits it\n",
    "\n",
    "sample_words = tokenize(\"system minors\")\n",
    "print(tfidf[dictionary.doc2bow(sample_words)]) # model is like a dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Corpora and Vector Spaces](https://radimrehurek.com/gensim/auto_examples/core/run_corpora_and_vector_spaces.html)\n",
    "\n",
    "Building of BOW \n",
    "\n",
    "Gensim only requires that a corpus must be able to return one document vector at a time, so vectors don't have to all reside in RAM at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1)]\n",
      "[(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)]\n",
      "[(2, 1), (5, 1), (7, 1), (8, 1)]\n",
      "[(1, 1), (5, 2), (8, 1)]\n",
      "[(3, 1), (6, 1), (7, 1)]\n",
      "[(9, 1)]\n",
      "[(9, 1), (10, 1)]\n",
      "[(9, 1), (10, 1), (11, 1)]\n",
      "[(4, 1), (10, 1), (11, 1)]\n"
     ]
    }
   ],
   "source": [
    "from smart_open import open # for transparently opening remote files\n",
    "\n",
    "URL = 'https://radimrehurek.com/mycorpus.txt'\n",
    "\n",
    "class MyCorpus:\n",
    "    def __iter__(self):\n",
    "        for line in open(URL):\n",
    "            yield dictionary.doc2bow(tokenize(line))\n",
    "\n",
    "corpus_ram_friendly = MyCorpus()\n",
    "for vec in corpus_ram_friendly:\n",
    "    print(vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "constructing the dictionary without loading all texts into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...)\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(tokenize(line) for line in open(URL))\n",
    "stop_ids = [\n",
    "    dictionary.token2id[sw]\n",
    "    for sw in stoplist\n",
    "    if sw in dictionary.token2id\n",
    "]\n",
    "once_ids = [tokenid for tokenid, docfreq in dictionary.dfs.items() if docfreq == 1]\n",
    "dictionary.filter_tokens(stop_ids+once_ids)\n",
    "dictionary.compactify() # remove gaps in id sequence after words that were removed\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we'd later need to do more transforms for this dictionary to be useful\n",
    "### Corpus formats\n",
    "serializing a Vector Space corpus to disk.\n",
    "\n",
    "Market Matrix format is common\n",
    "```python\n",
    "corpora.MmCorpus.serialize('/tmp/corpus.mm', corpus) #write\n",
    "corpus = corpora.MmCorpus('/tmp/corpus.mm') #read\n",
    "```\n",
    "convert to list to print it\n",
    "\n",
    "### NumPy + SciPy data structure integration\n",
    "```python\n",
    "gensim.matutils.Dense2Corpus\n",
    "gensime.matutils.corpus2dense\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topics and Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.7071067811865476), (1, 0.7071067811865476)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import models\n",
    "tfidf = models.TfidfModel(bow_corpus) #fit model\n",
    "\n",
    "sample_doc_bow = [(0, 1), (1, 1)]\n",
    "print(tfidf[sample_doc_bow])\n",
    "\n",
    "#entire corpus transform\n",
    "corpus_tfidf = tfidf[bow_corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding more transforms\n",
    "\n",
    "Dimensions are topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.703*\"trees\" + 0.538*\"graph\" + 0.402*\"minors\" + 0.187*\"survey\" + 0.061*\"system\" + 0.060*\"time\" + 0.060*\"response\" + 0.058*\"user\" + 0.049*\"computer\" + 0.035*\"interface\"'),\n",
       " (1,\n",
       "  '-0.460*\"system\" + -0.373*\"user\" + -0.332*\"eps\" + -0.328*\"interface\" + -0.320*\"response\" + -0.320*\"time\" + -0.293*\"computer\" + -0.280*\"human\" + -0.171*\"survey\" + 0.161*\"trees\"')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi_model = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=2) #map to 2d space\n",
    "corpus_lsi = lsi_model[corpus_tfidf]\n",
    "\n",
    "lsi_model.print_topics(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.06600783396090265), (1, -0.5200703306361851)] Human machine interface for lab abc computer applications\n",
      "[(0, 0.1966759285914235), (1, -0.760956316770005)] A survey of user opinion of computer system response time\n",
      "[(0, 0.08992639972446378), (1, -0.7241860626752508)] The EPS user interface management system\n",
      "[(0, 0.07585847652178145), (1, -0.6320551586003423)] System and human system engineering testing of EPS\n",
      "[(0, 0.1015029918498002), (1, -0.5737308483002957)] Relation of user perceived response time to error measurement\n",
      "[(0, 0.7032108939378315), (1, 0.1611518021402567)] The generation of random binary unordered trees\n",
      "[(0, 0.8774787673119835), (1, 0.16758906864659268)] The intersection graph of paths in trees\n",
      "[(0, 0.9098624686818582), (1, 0.14086553628718873)] Graph minors IV Widths of trees and well quasi ordering\n",
      "[(0, 0.6165825350569278), (1, -0.053929075663894585)] Graph minors A survey\n"
     ]
    }
   ],
   "source": [
    "# both bow->tfidf and tfidf->lsi transformations are actually executed here, on the fly\n",
    "for doc, as_text in zip(corpus_lsi, documents):\n",
    "    print(doc, as_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the first five documents are more strongly related to the second topic while the remaining four documents to the first topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "### model persistency\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "# creating tmp file just as an example\n",
    "with tempfile.NamedTemporaryFile(prefix='model-', suffix='.lsi', delete=False) as tmp:\n",
    "    lsi_model.save(tmp.name)\n",
    "    \n",
    "loaded_lsi_model = models.LsiModel.load(tmp.name)\n",
    "os.unlink(tmp.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some simple models\n",
    "\n",
    "\n",
    "- Tfidfmodel (term frequency-inverse document frequency): ranks lower frequency words higher \n",
    "- LSI/LSA: Best to do tfidf before. Ideal dimension is 200-500.\n",
    "    - online training:\n",
    "    ```python\n",
    "    model.add(another_tfidf_Corpus)\n",
    "    lsi_vec = model[tfidf_vec]\n",
    "    ```\n",
    "    - LSI can also [forget old obervsations](https://radimrehurek.com/gensim/models/lsimodel.html#module-gensim.models.lsimodel) in big streams\n",
    "- Random projections (RpModel)\n",
    "- Latent Dirichlet Allocation (LDA) : probablistic LSA (multinomial PCA). topics are interepreted as prob distribs over words.\n",
    "- Hierarchical Dirichlet Process (HDP) experimental\n",
    "\n",
    "Can also incorporate Value Stream Mapping(vsm) trasnformations, like different weighting schemes\n",
    "\n",
    "[example on wikpedia data](https://radimrehurek.com/gensim/wiki.html#wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
